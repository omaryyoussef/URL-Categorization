{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7305617faa11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdict_site\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdict_sites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named request"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "#from urllib import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "import time\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from dict_site import dict_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(898, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = 'Dataset.csv'\n",
    "dataset = pd.read_csv(mydata)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=898, step=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     https://www.4helal.tv/video/series-Rahim-01.html\n",
       "1       http://krmalk.tv/video/watch.php?vid=8e54b1d51\n",
       "2    https://www.mzarita.tv/video/watch.php?vid=dfa...\n",
       "3    https://www.mzarita.tv/video/watch.php?vid=157...\n",
       "4    https://www.mzarita.tv/video/watch.php?vid=c78...\n",
       "5    https://www.shahidwbas.com/watch.php?vid=b5ee3...\n",
       "6     https://www.rotana.video/watch.php?vid=a725098a5\n",
       "7    http://www.s7efty.com/video/watch.php?vid=93ef...\n",
       "8    https://cera.video/mosalsal-raheem-alhalqa-epi...\n",
       "9         https://qissat.video/watch.php?vid=094525f52\n",
       "Name: link, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['link'][1:10]\n",
    "# Better construction, you are already in pandas\n",
    "dataset.head(10)['link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping non working links from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_columns = np.append(dataset.columns.values,'source_site')\n",
    "filtered_dataset = pd.DataFrame(columns=dataset_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_source_name(url):\n",
    "        ''''\n",
    "        This function takes a url and returns the main domain. For example, if the input\n",
    "        is http://krmalk.tv/video/watch.php?vid=8e54b1d51, it returns krmalk.tv.\n",
    "        \n",
    "        @params:\n",
    "            url: A string the contains the url\n",
    "        @returns:\n",
    "            The main domain\n",
    "        ''''\n",
    "    return url.split('/')[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "447\n",
      "403\n",
      "403\n",
      "403\n",
      "500\n",
      "500\n",
      "403\n",
      "404\n",
      "403\n",
      "403\n",
      "403\n",
      "503\n",
      "403\n",
      "404\n",
      "404\n",
      "400\n",
      "400\n",
      "400\n",
      "503\n",
      "404\n",
      "522\n",
      "400\n",
      "503\n",
      "500\n",
      "403\n",
      "404\n",
      "404\n",
      "404\n",
      "503\n",
      "503\n",
      "404\n",
      "404\n",
      "522\n",
      "404\n",
      "403\n",
      "404\n",
      "403\n",
      "503\n",
      "404\n",
      "404\n",
      "404\n",
      "403\n",
      "403\n",
      "404\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "get_error = []\n",
    "broken_links = []\n",
    "\n",
    "# Iterating through the links in the dataset\n",
    "for index, row in dataset.iterrows():\n",
    "    \n",
    "    # The try except is used to catch a connection peer error that happens for some websites.\n",
    "    try:\n",
    "        # Get request for the link\n",
    "        response = requests.get(row['link'], allow_redirects=True)\n",
    "    except:\n",
    "        get_error.append(row['link'])\n",
    "        continue\n",
    "    \n",
    "    # Conditioning if the status code is 200 which means that the request has succeeded\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        source_site = get_source_name(row['link'])\n",
    "        filtered_dataset = filtered_dataset.append({'link': row['link'],'class':row['class'],'source_site':source_site}, ignore_index=True)\n",
    "        continue\n",
    "        \n",
    "    # If the request did not succeed, save those links and the their status codes  \n",
    "    else:\n",
    "        broken_links.append([row['link'],response.status_code])\n",
    "        print(response.status_code)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking only the websites that have a frequency of 5 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site_count = filtered_dataset.groupby(['source_site'])\\\n",
    ".count()\\\n",
    ".sort_values(by='class',ascending=False)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sites_with_atleast_5 = site_count.loc[site_count['class']>=5]\\\n",
    ".reset_index()\\\n",
    "['source_site']\\\n",
    ".unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_filtered_dataset = filtered_dataset.loc[filtered_dataset['source_site'].isin(sites_with_atleast_5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the new balance of the final filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 308, 1: 333})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(final_filtered_dataset['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage amount of data taken when thresholding by 5 is :  79.82565379825654\n"
     ]
    }
   ],
   "source": [
    "print(\"The percentage amount of data taken when thresholding by 5 is : \", len(final_filtered_dataset)/len(filtered_dataset)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the information that are required to scrape the website are inside the **dict_sites** dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the features from the websites, I decided to scrape the website directly, regardless whether the URL displays some information or not, this is because for some websites the URL is not consistent.\n",
    ">For example we might have:\n",
    "    - ramdan.video/video/rahim+episode+5\n",
    "    - ramdan.video/video/123we141\n",
    "    \n",
    "So either way I am going to scrape them, to extract information for the second type of format, hence using scraping for all the domains is a better and safer approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain **www.wataan.com** is troublesome because it returns weird symbols because of page encoding issues. It is most probably not set to UTF-8 and so it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_filtered_dataset = final_filtered_dataset.loc[~(final_filtered_dataset['source_site']=='www.wataan.com')]\n",
    "series_variations = ['raheem', 'rahim', 'ra7em', 'ra7eem', 'ra7im', 'رحيم', 'r7eem', 'rahiem','ra7iem', 'ر7يم']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_filtered_dataset = final_filtered_dataset.loc[~(final_filtered_dataset['source_site']=='medium.com')]\n",
    "final_filtered_dataset = final_filtered_dataset.loc[~(final_filtered_dataset['source_site']=='www.youbox7.com')]\n",
    "final_filtered_dataset = final_filtered_dataset.loc[~(final_filtered_dataset['source_site']=='www.halacima.net')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>class</th>\n",
       "      <th>source_site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.4helal.tv/video/series-Rahim-01.html</td>\n",
       "      <td>1</td>\n",
       "      <td>www.4helal.tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://krmalk.tv/video/watch.php?vid=8e54b1d51</td>\n",
       "      <td>1</td>\n",
       "      <td>krmalk.tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.mzarita.tv/video/watch.php?vid=dfa...</td>\n",
       "      <td>1</td>\n",
       "      <td>www.mzarita.tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.mzarita.tv/video/watch.php?vid=157...</td>\n",
       "      <td>1</td>\n",
       "      <td>www.mzarita.tv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.mzarita.tv/video/watch.php?vid=c78...</td>\n",
       "      <td>1</td>\n",
       "      <td>www.mzarita.tv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link class     source_site\n",
       "0   https://www.4helal.tv/video/series-Rahim-01.html     1   www.4helal.tv\n",
       "1     http://krmalk.tv/video/watch.php?vid=8e54b1d51     1       krmalk.tv\n",
       "2  https://www.mzarita.tv/video/watch.php?vid=dfa...     1  www.mzarita.tv\n",
       "3  https://www.mzarita.tv/video/watch.php?vid=157...     1  www.mzarita.tv\n",
       "4  https://www.mzarita.tv/video/watch.php?vid=c78...     1  www.mzarita.tv"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_filtered_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shahidline.com', 'tv.alarab.com', 'www.elrdar.com', 'www.kelmeten.com'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(list(final_filtered_dataset['source_site'].unique()))\n",
    "b = set(list(dict_sites.keys()))\n",
    "a-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sites should be in the if condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'www.youbox7.com'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c4fb1ca0257e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Used to scrape for each site\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mattr_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mattr_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_site\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attr_value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'www.youbox7.com'"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "\n",
    "for index,row in final_filtered_dataset.iterrows():\n",
    "    \n",
    "    source_site = row['source_site']\n",
    "    \n",
    "    # Requesting the site and scraping\n",
    "    r = requests.get(row['link'])\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # Special cases for scraping\n",
    "    \n",
    "    if source_site == 'tv.alarab.com':\n",
    "        try:\n",
    "            title = soup.find('div', attrs={'id':'banner_slider'}).find('h2').text\n",
    "            titles.append(title)\n",
    "            continue\n",
    "        except:\n",
    "            titles.append('Cant scrape')\n",
    "            continue\n",
    "        \n",
    "    elif source_site == 'www.elrdar.com':\n",
    "        try:\n",
    "            title = soup.find('div', attrs={'class':'col-xs-12 col-sm-12 col-md-10'}).find('h1').text\n",
    "            titles.append(title)\n",
    "            continue\n",
    "        except:\n",
    "            titles.append('Cant scrape')\n",
    "            continue\n",
    "            \n",
    "    elif source_site == 'www.mosalslat.video':\n",
    "        try:\n",
    "            title = soup.find('h1', attrs={'class':'entry-title'}).text\n",
    "            titles.append(title)\n",
    "            continue\n",
    "        except:\n",
    "            titles.append('Cant scrape')\n",
    "            continue\n",
    "            \n",
    "    elif source_site == 'www.kelmeten.com':\n",
    "        try:\n",
    "            title = soup.find('div', attrs={'class':'row pm-video-heading'}).find('h1').text\n",
    "            titles.append(title)\n",
    "            continue\n",
    "        except:\n",
    "            titles.append('Cant scrape')\n",
    "            continue\n",
    "            \n",
    "    elif source_site == 'shahidline.com':\n",
    "        try:\n",
    "            title = soup.find('div', attrs={'class':'on-episode-postinfo'}).find('h1').text\n",
    "            titles.append(title)\n",
    "            continue\n",
    "        except:\n",
    "            titles.append('Cant scrape')\n",
    "            continue\n",
    "            \n",
    "    # Used to scrape for each site\n",
    "    attr_class = dict_sites[source_site]['attr_class']\n",
    "    attr_type = dict_sites[source_site]['attr_type']\n",
    "    attr_value = dict_sites[source_site]['attr_value']\n",
    "    \n",
    "    try:\n",
    "        title = soup.find(attr_type, attrs={attr_class:attr_value}).text\n",
    "        titles.append(title)\n",
    "    except:\n",
    "        titles.append('Cant scrape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If any of the words is present in the title, return 1, else return 0 \n",
    "is_present = []\n",
    "for title in titles:\n",
    "    if any(word in title for word in series_variations):\n",
    "        is_present.append(1)\n",
    "    else:\n",
    "        is_present.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_filtered_dataset['titles'] = titles\n",
    "final_filtered_dataset['is_present'] = is_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abs(final_filtered_dataset['is_present'] - final_filtered_dataset['class']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook main.ipynb to python\n",
      "[NbConvertApp] Writing 7241 bytes to main.py\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbconvert --to=python main.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
